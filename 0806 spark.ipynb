{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.16.132.218:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x144863b45c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.16.132.218:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sample/sample/ages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes   #자동으로 스키마를 적용해 dataframe을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"sample/sample/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"sample/sample/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\":\"Michael\"}',\n",
       " '{\"name\":\"Andy\", \"age\":30}',\n",
       " '{\"name\":\"Justin\", \"age\":19}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()    #데이터를 스키마 없이 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd)    #rdd로부터 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize((\"\"\"\n",
    "    {\n",
    "        \"id\": \"123\",\n",
    "        \"name\": \"Katie\",\n",
    "        \"age\": 19,\n",
    "        \"eyeColor\": \"brown\"\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "        \"id\": \"234\",\n",
    "        \"name\": \"Michael\",\n",
    "        \"age\": 22,\n",
    "        \"eyeColor\": \"green\"\n",
    "    }\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "    {\n",
    "        \"id\": \"345\",\n",
    "        \"name\": \"Simone\",\n",
    "        \"age\": 23,\n",
    "        \"eyeColor\": \"blue\"\n",
    "    }\n",
    "\"\"\"              \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    {\\n        \"id\": \"123\",\\n        \"name\": \"Katie\",\\n        \"age\": 19,\\n        \"eyeColor\": \"brown\"\\n    }\\n',\n",
       " '\\n    {\\n        \"id\": \"234\",\\n        \"name\": \"Michael\",\\n        \"age\": 22,\\n        \"eyeColor\": \"green\"\\n    }\\n',\n",
       " '\\n    {\\n        \"id\": \"345\",\\n        \"name\": \"Simone\",\\n        \"age\": 23,\\n        \"eyeColor\": \"blue\"\\n    }\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, eyeColor: string, id: string, name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"test\")   #스키마 구조를 갖고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()    #row의 갯수 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id','age').filter(\"age=22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, age from test where age=22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.age).filter(df.age == 22).show()    #파이썬 문법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\n",
    "    [\n",
    "        (123, \"Katie\", 19, \"brown\"),\n",
    "        (234, \"Michael\", 22, \"green\"),\n",
    "        (345, \"Simone\", 23, \"blue\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = StructType(\n",
    "    [\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), nullable=True),\n",
    "        StructField(\"age\", LongType(), nullable=True),\n",
    "        StructField(\"eyeColor\", StringType(), nullable=True)       \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=123, name='Katie', age=19, eyeColor='brown'),\n",
       " Row(id=234, name='Michael', age=22, eyeColor='green'),\n",
       " Row(id=345, name='Simone', age=23, eyeColor='blue')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from test2\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 'Katie', 19, 'brown'),\n",
       " (234, 'Michael', 22, 'green'),\n",
       " (345, 'Simone', 23, 'blue')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=123, name='Katie', age=19, eyeColor='brown'),\n",
       " Row(id=234, name='Michael', age=22, eyeColor='green'),\n",
       " Row(id=345, name='Simone', age=23, eyeColor='blue')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightPath = \"sample/departuredelays.csv\"\n",
    "airportPath = \"sample/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = spark.read.csv(flightPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='01011245', delay='6', distance='602', origin='ABE', destination='ATL')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = spark.read.csv(airportPath, header = True, inferSchema=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abbotsford', State='BC', Country='Canada', IATA='YXX')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight.createOrReplaceTempView(\"flight\")\n",
    "airport.createOrReplaceTempView(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|   delay|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select a.City, f.origin, sum(f.delay) as delay \n",
    "        from flight f \n",
    "            join  airport a \n",
    "                on a.IATA = f.origin\n",
    "    where a.State = \"WA\"\n",
    "    group by a.City, f.origin\n",
    "    order by delay desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   City|origin|sum(delay)|\n",
      "+-------+------+----------+\n",
      "|Seattle|   SEA|  159086.0|\n",
      "|Spokane|   GEG|   12404.0|\n",
      "|  Pasco|   PSC|     949.0|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport.join(flight, airport.IATA==flight.origin).where(airport.State ==\"WA\").select(airport.City, flight.origin, flight.delay)\\\n",
    "        .groupBy(airport.City, flight.origin).agg(F.sum(flight.delay))\\\n",
    "        .orderBy(\"sum(delay)\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "streaming을 통해 실시간으로 데이터를 받아와 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = StreamingContext(sc, 5) #5초마다 streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.streaming.context.StreamingContext at 0x14487891860>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어디에서 streaming을 받아올 것인지 socket 설정\n",
    "lines = streaming.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)   #rdd 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line:line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda w:(w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pairs.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "awaitTerminationOrTimeout() missing 1 required positional argument: 'timeout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-36b5f277b851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstreaming\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: awaitTerminationOrTimeout() missing 1 required positional argument: 'timeout'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:45\n",
      "-------------------------------------------\n",
      "('abc', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:50\n",
      "-------------------------------------------\n",
      "('ababab', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:08:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:00\n",
      "-------------------------------------------\n",
      "('exrer', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:05\n",
      "-------------------------------------------\n",
      "('adfdsfsd', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-06 14:09:55\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.start()\n",
    "streaming.awaitTerminationOrTimeout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming.start()\n",
    "streaming.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.readStream.format(\"socket\")\\\n",
    "    .option(\"host\", \"localhost\")\\\n",
    "    .option(\"port\", 9999)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "    explode(split(lines.value, \" \")).alias(\"word\")   #각 line의 내용을 space로 split후 word로 명명\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = wordCount.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 144.5, 5.9, 33, \"M\"),\n",
    "        (2, 167.2, 5.4, 45, \"M\"),\n",
    "        (3, 124.1, 5.2, 23, \"F\"),\n",
    "        (4, 144.5, 5.9, 33, \"M\"),\n",
    "        (5, 133.2, 5.7, 54, \"F\"),\n",
    "        (3, 124.1, 5.2, 23, \"F\"),\n",
    "        (5, 129.2, 5.3, 42, \"M\")        \n",
    "    ],\n",
    "    [\"id\",\"weight\", \"height\",\"age\",\"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropDuplicates([col for col in df.columns if col != \"id\"])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([col for col in df.columns if col == \"id\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Count|DCount|\n",
      "+-----+------+\n",
      "|    5|     4|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    F.count(\"id\").alias(\"Count\"), \n",
    "    F.countDistinct('id').alias(\"DCount\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|          nid|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"nid\", F.monotonically_increasing_id()).show()   #nid 라는 새로운 column을 만든 후 새로운 id 부여(col 명이 이미 존재한다면 update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+---+\n",
      "| id|weight|height|age|gender|old|\n",
      "+---+------+------+---+------+---+\n",
      "|  5| 133.2|   5.7| 54|     F|  5|\n",
      "|  1| 144.5|   5.9| 33|     M|  1|\n",
      "|  2| 167.2|   5.4| 45|     M|  2|\n",
      "|  3| 124.1|   5.2| 23|     F|  3|\n",
      "|  5| 129.2|   5.3| 42|     M|  5|\n",
      "+---+------+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"old\", df.id)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+---+------+---+\n",
      "|           id|weight|height|age|gender|old|\n",
      "+-------------+------+------+---+------+---+\n",
      "|  25769803776| 133.2|   5.7| 54|     F|  5|\n",
      "| 171798691840| 144.5|   5.9| 33|     M|  1|\n",
      "| 592705486848| 167.2|   5.4| 45|     M|  2|\n",
      "|1236950581248| 124.1|   5.2| 23|     F|  3|\n",
      "|1365799600128| 129.2|   5.3| 42|     M|  5|\n",
      "+-------------+------+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+---+------+---+\n",
      "|           id|weight|height|age|gender|old|\n",
      "+-------------+------+------+---+------+---+\n",
      "| 171798691840| 144.5|   5.9| 33|     M|  1|\n",
      "| 592705486848| 167.2|   5.4| 45|     M|  2|\n",
      "|1236950581248| 124.1|   5.2| 23|     F|  3|\n",
      "|  25769803776| 133.2|   5.7| 54|     F|  5|\n",
      "|1365799600128| 129.2|   5.3| 42|     M|  5|\n",
      "+-------------+------+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"old\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.6, 28, \"M\", 100000),\n",
    "        (2, 167.2,  5.4,45, \"M\", None),\n",
    "        (3, None, 5.2, None, None, None),\n",
    "        (4, 144.5, 5.9, 33, \"M\", None),\n",
    "        (5, 133.2, 5.7, 54, \"F\", None),\n",
    "        (6, 124.1, 5.2, None, \"F\", None),\n",
    "        (7, 129.2, 5.3, 42, \"M\", 76000)\n",
    "    ],\n",
    "    \n",
    "    [\"id\",\"weight\", \"height\",\"age\",\"gender\", \"income\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(\n",
    "    lambda row:(row[\"id\"], sum([c==None for c in row]))    #row 별 missing value count\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"id=3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "|idRate|        weightRate|heightRate|           ageRate|        genderRate|        incomeRate|\n",
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "|   0.0|0.1428571428571429|       0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+------+------------------+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*[(1 - F.count(c)/F.count(\"*\")).alias(c+\"Rate\") for c in df.columns]).show()    #missing data가 있는지 missing 비율을 통해 판별 - rate가 높으면 데이터로써 가치 적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([c for c in df.columns if c!= \"income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(thresh=3).show() #null값이 한 row에 tresh 이상만큼 있는 row를 날림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 col에 null 값이 많지 않은 경우 전체 row의 평균값으로 null값을 채워줌 -> 후에 통계분석할 때 영향을 주지 않도록\n",
    "means = df.agg(\n",
    "    *[F.mean(c).alias(c) for c in df.columns if c != \"gender\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+----+\n",
      "| id|            weight|           height| age|\n",
      "+---+------------------+-----------------+----+\n",
      "|4.0|140.28333333333333|5.471428571428571|40.4|\n",
      "+---+------------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "means.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = means.toPandas()    #dataframe 은 immutable이기 때문에 pandas를 애용해서 데이터 수정 후 새로운 dataframe으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>140.283333</td>\n",
       "      <td>5.471429</td>\n",
       "      <td>40.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      weight    height   age\n",
       "0  4.0  140.283333  5.471429  40.4"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = mpd.to_dict(\"records\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd['gender'] = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4,\n",
       " 'gender': 'X'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+------+\n",
      "| id|            weight|height|age|gender|\n",
      "+---+------------------+------+---+------+\n",
      "|  1|             143.5|   5.6| 28|     M|\n",
      "|  2|             167.2|   5.4| 45|     M|\n",
      "|  3|140.28333333333333|   5.2| 40|     X|\n",
      "|  4|             144.5|   5.9| 33|     M|\n",
      "|  5|             133.2|   5.7| 54|     F|\n",
      "|  6|             124.1|   5.2| 40|     F|\n",
      "|  7|             129.2|   5.3| 42|     M|\n",
      "+---+------------------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.3, 28),\n",
    "        (2, 154.2,  5.5, 45),\n",
    "        (3, 342.3, 5.2, 99),\n",
    "        (4, 144.5, 5.5, 33),\n",
    "        (5, 133.2, 5.4, 54),\n",
    "        (6, 124.1, 5.1, 21),\n",
    "        (7, 129.2, 5.3, 42)\n",
    "    ],\n",
    "    \n",
    "    [\"id\",\"weight\", \"height\",\"age\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.approxQuantile(\"weight\", [0.25, 0.75], 0.05)  #relativeError = 0 일수록 해당 data에 specific -> 어느 정도 오류를 허용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[129.2, 154.2]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = quantiles[1] - quantiles[0]   #중간값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [quantiles[0]-1.5*IQR , quantiles[1]+1.5*IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91.69999999999999, 191.7]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"weight\", \"height\", \"age\"]\n",
    "bounds = {}\n",
    "\n",
    "for c in col:\n",
    "    quantiles = df.approxQuantile(c, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[c] = [quantiles[0]-1.5*IQR , quantiles[1]+1.5*IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': [91.69999999999999, 191.7],\n",
       " 'height': [4.75, 5.949999999999999],\n",
       " 'age': [-11.0, 93.0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|(weight < 91.69999999999999)|\n",
      "+----------------------------+\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "+----------------------------+\n",
      "\n",
      "+----------------+\n",
      "|(weight > 191.7)|\n",
      "+----------------+\n",
      "|           false|\n",
      "|           false|\n",
      "|            true|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "|           false|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"weight\"] < bounds[\"weight\"][0]).show()\n",
    "df.select(df[\"weight\"] > bounds[\"weight\"][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier = df.select(\n",
    "    *[\"id\"] + [( (df[c] < bounds[c][0]) |( df[c] > bounds[c][1])).alias(c+\"_o\") for c in col]\n",
    ")\n",
    "outlier.show()\n",
    "#true이면 outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+--------+--------+-----+\n",
      "| id|weight|height|age|weight_o|height_o|age_o|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "|  7| 129.2|   5.3| 42|   false|   false|false|\n",
      "|  6| 124.1|   5.1| 21|   false|   false|false|\n",
      "|  5| 133.2|   5.4| 54|   false|   false|false|\n",
      "|  1| 143.5|   5.3| 28|   false|   false|false|\n",
      "|  3| 342.3|   5.2| 99|    true|   false| true|\n",
      "|  2| 154.2|   5.5| 45|   false|   false|false|\n",
      "|  4| 144.5|   5.5| 33|   false|   false|false|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinTable = df.join(outlier, on=\"id\")\n",
    "joinTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  7| 129.2|   5.3| 42|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinTable.filter(\"!weight_o and !height_o and !age_o\").select(\"id\", \"weight\", \"height\",\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
